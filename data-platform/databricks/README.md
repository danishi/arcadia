# ARCADIA Data Platform - Databricks Implementation

Databricks データインテリジェンスプラットフォーム上で Medallion Architecture を構築するためのスクリプトテンプレート集。

## 前提条件

- Databricks Workspace が利用可能であること
- Unity Catalog が有効化されていること
- Serverless SQL Warehouse が利用可能であること（06_setup_analytics で使用）
- Python 3.10+ がインストールされていること

## 環境変数

| 変数名 | 説明 | 例 |
|--------|------|-----|
| `DATABRICKS_HOST` | Workspace URL | `https://xxx.cloud.databricks.com` |
| `DATABRICKS_TOKEN` | Personal Access Token | `dapi...` |
| `DATABRICKS_WAREHOUSE_ID` | SQL Warehouse ID | `abc123def456` |

## スクリプト一覧

| # | ファイル | 内容 | 目安時間 |
|---|---------|------|---------|
| 01 | `01_setup_infrastructure.py.tmpl` | Unity Catalog / Schema / Volume 作成 | 10秒 |
| 03 | `03_bronze_ingestion.py.tmpl` | CSV/JSON -> Bronze テーブル取込 (COPY INTO) | 30秒 |
| 04 | `04_silver_transform.py.tmpl` | Bronze -> Silver 変換 (MERGE INTO) | 60秒 |
| 05 | `05_gold_aggregate.py.tmpl` | Silver -> Gold 集約 (CTAS) | 60秒 |
| 06 | `06_setup_analytics.py.tmpl` | SQL クエリ検証 / Genie Space / Dashboard 準備 | 30秒 |

## Medallion Architecture on Databricks

```
Unity Catalog Volume (S3/ADLS/GCS)
    |  CSV / JSON files
    v
+-------------------+     COPY INTO / Auto Loader
|  Bronze (Delta)   | <-- raw_customers, raw_accounts, ...
+-------------------+
    |  MERGE INTO (dedup, type cast, null handling)
    v
+-------------------+
|  Silver (Delta)   | <-- customers, accounts, transactions, ...
+-------------------+
    |  CTAS with JOINs & aggregations
    v
+-------------------+
|  Gold (Delta)     | <-- customer_360, campaign_effectiveness, ...
+-------------------+
    |
    v
+-------------------+
|  Genie Space      | --> Natural language queries
|  AI/BI Dashboard  | --> Visual analytics
+-------------------+
```

## セットアップ手順

### 1. テンプレートの準備

```bash
# common/ の設定ファイルをコピー・編集
cp ../common/00_config.py.tmpl ../common/00_config.py
# PROJECT_SLUG, PLATFORM_TYPE="databricks", CATALOG_LOCATION を設定

# Databricks スクリプトのテンプレートをコピー
for f in *.py.tmpl; do cp "$f" "${f%.tmpl}"; done
```

### 2. プレースホルダの置換

各 `.py` ファイル内の `{{PLACEHOLDER}}` を実際の値に置換する。

| プレースホルダ | 説明 |
|---------------|------|
| `{{PROJECT_SLUG}}` | プロジェクト識別子 |
| `{{CATALOG_LOCATION}}` | Unity Catalog マネージドロケーション (S3/ADLS パス) |

### 3. Databricks Notebook での実行

これらのスクリプトは Databricks Notebook 形式 (`# Databricks notebook source`) に変換して使用することも可能。

```python
# Notebook セルの先頭に追加
# MAGIC %run ../common/00_config
```

### 4. ワンクリック実行

```bash
# 全パイプラインを順次実行
python ../common/07_run_all.py
```

## Databricks 固有の概念マッピング

| 汎用概念 | Databricks 実装 |
|---------|----------------|
| Catalog | Unity Catalog |
| Schema | Unity Catalog Schema |
| Landing Zone | Unity Catalog Volume |
| Raw Ingestion | COPY INTO / Auto Loader (cloudFiles) |
| Incremental Update | MERGE INTO (Delta Lake) |
| Table Format | Delta Lake |
| Data Quality | DLT Expectations / custom SQL checks |
| Clustering | Liquid Clustering (CLUSTER BY) |
| Query Engine | Serverless SQL Warehouse |
| NL Analytics | Genie Space |
| Dashboard | AI/BI Dashboard |
| ML Platform | Mosaic AI / MLflow |
| Governance | Unity Catalog ACL / Row/Column Filters |

## 本番移行時の推奨事項

1. **Auto Loader への切替**: デモでは COPY INTO を使用しているが、本番では Auto Loader (cloudFiles) を推奨
2. **Delta Live Tables (DLT)**: Bronze -> Silver -> Gold のパイプラインを DLT で宣言的に管理
3. **Liquid Clustering**: 大規模テーブルにはパーティション代わりに CLUSTER BY を使用
4. **Change Data Capture (CDC)**: MERGE INTO のソースを CDC ストリームに置換
5. **Governance**: Unity Catalog の行フィルタ・列マスクで PII を保護

---

*Generated by ARCADIA Framework - Phase F: Data Platform Templates (Databricks)*
