# ==============================================================================
# ARCADIA Data Platform - Databricks Bronze Ingestion
# Generated by ARCADIA Framework (Phase F)
#
# Ingests CSV/JSON files from Unity Catalog Volume into Bronze Delta tables.
#
# Method: COPY INTO (idempotent, incremental)
#   - Already-ingested files are skipped automatically
#   - Schema is inferred from source files
#
# Production recommendation: Use Auto Loader (cloudFiles) instead of COPY INTO
#   for streaming ingestion with automatic schema evolution.
# ==============================================================================

# Databricks notebook source

# MAGIC %md
# MAGIC # 03. Bronze Layer Ingestion (Raw -> Bronze)
# MAGIC Ingest CSV/JSON files from Volume into Bronze Delta tables.
# MAGIC
# MAGIC **Method: COPY INTO** (idempotent, incremental)
# MAGIC
# MAGIC > For production, consider Auto Loader (cloudFiles) for streaming ingestion.

# COMMAND ----------

# MAGIC %run ../common/00_config

# COMMAND ----------

CATALOG = catalog_name()
VOL_PATH = volume_path()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. CSV Table Ingestion

# COMMAND ----------

# CUSTOMIZE: Add or modify table-file mappings for your data sources
csv_tables = [
    ("customers",          "customers/customers.csv"),
    ("accounts",           "accounts/accounts.csv"),
    ("transactions",       "transactions/transactions.csv"),
    ("campaigns",          "campaigns/campaigns.csv"),
    ("campaign_responses", "campaign_responses/campaign_responses.csv"),
]

for table_name, file_path in csv_tables:
    full_table = fqn(SCHEMA_BRONZE, f"raw_{table_name}")
    full_path = f"{VOL_PATH}/{file_path}"

    # Demo: Drop and recreate for clean ingestion each time
    # Production: Remove DROP TABLE and rely on COPY INTO's idempotency
    spark.sql(f"DROP TABLE IF EXISTS {full_table}")
    spark.sql(f"CREATE TABLE IF NOT EXISTS {full_table}")

    spark.sql(f"""
        COPY INTO {full_table}
        FROM '{full_path}'
        FILEFORMAT = CSV
        FORMAT_OPTIONS (
            'header' = 'true',
            'inferSchema' = 'true'
        )
        COPY_OPTIONS ('mergeSchema' = 'true')
    """)
    count = spark.table(full_table).count()
    print_status("CSV", f"{full_table}: {count:,} records ingested")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. JSON Table Ingestion (Web Events)

# COMMAND ----------

full_table = fqn(SCHEMA_BRONZE, "raw_web_events")
full_path = f"{VOL_PATH}/web_events/web_events.json"

# Demo: Drop and recreate for clean ingestion
spark.sql(f"DROP TABLE IF EXISTS {full_table}")
spark.sql(f"CREATE TABLE IF NOT EXISTS {full_table}")

spark.sql(f"""
    COPY INTO {full_table}
    FROM '{full_path}'
    FILEFORMAT = JSON
    FORMAT_OPTIONS (
        'inferSchema' = 'true'
    )
    COPY_OPTIONS ('mergeSchema' = 'true')
""")
count = spark.table(full_table).count()
print_status("JSON", f"{full_table}: {count:,} records ingested")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Ingestion Summary

# COMMAND ----------

print("=" * 60)
print("  Bronze Layer - Ingestion Complete")
print("=" * 60)

for t in BRONZE_TABLES:
    full_name = fqn(SCHEMA_BRONZE, t)
    cnt = spark.table(full_name).count()
    print(f"  {t:<28s} : {cnt:>8,} records")

print("=" * 60)

# ==============================================================================
# Auto Loader alternative (production recommendation)
# ==============================================================================
# To use Auto Loader instead of COPY INTO, replace the ingestion with:
#
# df = (spark.readStream
#     .format("cloudFiles")
#     .option("cloudFiles.format", "csv")
#     .option("cloudFiles.schemaLocation", f"{VOL_PATH}/_schemas/{table_name}")
#     .option("cloudFiles.inferColumnTypes", "true")
#     .option("header", "true")
#     .load(f"{VOL_PATH}/{subdir}/")
# )
#
# (df.writeStream
#     .option("checkpointLocation", f"{VOL_PATH}/_checkpoints/{table_name}")
#     .trigger(availableNow=True)
#     .toTable(fqn(SCHEMA_BRONZE, f"raw_{table_name}"))
# )
