# ==============================================================================
# ARCADIA Data Platform - Orchestrator
# Generated by ARCADIA Framework (Phase F)
#
# Detects PLATFORM_TYPE from config and executes the corresponding
# platform-specific scripts in sequence.
#
# Usage:
#   python 07_run_all.py
#
# Execution order:
#   1. config          - Load shared configuration
#   2. infrastructure  - Create catalog/schema/storage
#   3. data_gen        - Generate sample data (CSV/JSON)
#   4. bronze          - Ingest raw files into Bronze layer
#   5. silver          - Transform Bronze -> Silver
#   6. gold            - Aggregate Silver -> Gold
#   7. analytics       - Validate queries / prepare dashboards
# ==============================================================================

import os
import sys
import time
import subprocess

# Add common/ to path for config import
COMMON_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, COMMON_DIR)

try:
    from importlib import import_module
    config = import_module("00_config")
    PROJECT_SLUG = config.PROJECT_SLUG
    PLATFORM_TYPE = config.PLATFORM_TYPE
    print_status = config.print_status
except ImportError:
    print("[ERROR] 00_config.py not found. Copy 00_config.py.tmpl to 00_config.py and configure it.")
    sys.exit(1)


# ==============================================================================
# Platform script mapping
# ==============================================================================

PLATFORM_DIR = os.path.join(os.path.dirname(COMMON_DIR), PLATFORM_TYPE)

# Script execution order: (step_name, script_location, description)
PIPELINE_STEPS = [
    # Step 1: Infrastructure (platform-specific)
    (
        "infrastructure",
        os.path.join(PLATFORM_DIR, "01_setup_infrastructure.py"),
        "Create catalog / schema / storage",
    ),
    # Step 2: Sample data generation (common)
    (
        "data_gen",
        os.path.join(COMMON_DIR, "02_generate_sample_data.py"),
        "Generate sample data (CSV/JSON)",
    ),
    # Step 3: Bronze ingestion (platform-specific)
    (
        "bronze",
        os.path.join(PLATFORM_DIR, "03_bronze_ingestion.py"),
        "Ingest raw files -> Bronze layer",
    ),
    # Step 4: Silver transformation (platform-specific)
    (
        "silver",
        os.path.join(PLATFORM_DIR, "04_silver_transform.py"),
        "Transform Bronze -> Silver",
    ),
    # Step 5: Gold aggregation (platform-specific)
    (
        "gold",
        os.path.join(PLATFORM_DIR, "05_gold_aggregate.py"),
        "Aggregate Silver -> Gold",
    ),
    # Step 6: Analytics setup (platform-specific)
    (
        "analytics",
        os.path.join(PLATFORM_DIR, "06_setup_analytics.py"),
        "Validate queries / prepare dashboards",
    ),
]


def run_step(step_name: str, script_path: str, description: str) -> tuple[bool, float]:
    """Execute a single pipeline step.

    Returns:
        (success, elapsed_seconds)
    """
    print(f"\n--- Step: {step_name} ---")
    print(f"    {description}")
    print(f"    Script: {script_path}")

    if not os.path.exists(script_path):
        print(f"    [SKIP] Script not found: {script_path}")
        return True, 0.0

    start = time.time()
    try:
        result = subprocess.run(
            [sys.executable, script_path],
            capture_output=False,
            text=True,
            cwd=os.path.dirname(script_path),
        )
        elapsed = time.time() - start
        if result.returncode != 0:
            print_status(step_name, f"FAILED (exit code {result.returncode})", success=False)
            return False, elapsed
        print_status(step_name, f"Completed in {elapsed:.1f}s")
        return True, elapsed
    except Exception as e:
        elapsed = time.time() - start
        print_status(step_name, f"ERROR: {e}", success=False)
        return False, elapsed


def main():
    print("=" * 60)
    print(f"  ARCADIA Data Platform - Pipeline Orchestrator")
    print(f"  Project  : {PROJECT_SLUG}")
    print(f"  Platform : {PLATFORM_TYPE}")
    print("=" * 60)

    # Validate platform directory exists
    if not os.path.isdir(PLATFORM_DIR):
        print(f"\n[ERROR] Platform directory not found: {PLATFORM_DIR}")
        print(f"        Supported platforms: databricks, snowflake, bigquery")
        sys.exit(1)

    total_start = time.time()
    results = []

    for step_name, script_path, description in PIPELINE_STEPS:
        success, elapsed = run_step(step_name, script_path, description)
        results.append((step_name, success, elapsed))
        if not success:
            print(f"\n[ERROR] Pipeline failed at step: {step_name}")
            print(f"        Fix the issue and re-run.")
            break

    total_elapsed = time.time() - total_start
    minutes = int(total_elapsed // 60)
    seconds = int(total_elapsed % 60)

    # Summary
    print("\n" + "=" * 60)
    print(f"  Pipeline Execution Summary")
    print("=" * 60)
    print(f"  Project  : {PROJECT_SLUG}")
    print(f"  Platform : {PLATFORM_TYPE}")
    print(f"  Duration : {minutes}m {seconds}s")
    print()

    all_ok = True
    for step_name, success, elapsed in results:
        status = "[OK]" if success else "[NG]"
        print(f"  {status} {step_name:<20s} {elapsed:>6.1f}s")
        if not success:
            all_ok = False

    print("=" * 60)

    if all_ok:
        print("\n  Pipeline completed successfully.")
        print(f"\n  Next steps:")
        print(f"  1. Start your SQL Warehouse / Compute")
        print(f"  2. Create a natural language analysis space (Genie / Cortex / Gemini)")
        print(f"  3. Build dashboards from Gold layer tables")
    else:
        print("\n  Pipeline completed with errors. Check logs above.")
        sys.exit(1)

    print("=" * 60)


if __name__ == "__main__":
    main()
