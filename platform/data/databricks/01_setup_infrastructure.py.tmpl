# ==============================================================================
# ARCADIA Data Platform - Databricks Infrastructure Setup
# Generated by ARCADIA Framework (Phase F)
#
# Creates Unity Catalog, Schemas (Medallion 3-layer), and Volumes idempotently.
# Safe to run multiple times (IF NOT EXISTS).
#
# Databricks Notebook Usage:
#   # MAGIC %run ../common/00_config
#
# Standalone Usage:
#   Requires databricks-sdk: pip install databricks-sdk
# ==============================================================================

# Databricks notebook source

# MAGIC %md
# MAGIC # 01. Infrastructure Setup
# MAGIC Create Catalog, Schemas (Bronze/Silver/Gold), and Volume for raw data landing.
# MAGIC Idempotent - safe to run multiple times.

# COMMAND ----------

# MAGIC %run ../common/00_config

# COMMAND ----------

# ==============================================================================
# Configuration (from common/00_config.py)
# ==============================================================================
# CATALOG:          {{PROJECT_SLUG}}_demo
# CATALOG_LOCATION: {{CATALOG_LOCATION}}
# SCHEMAS:          bronze, silver, gold
# VOLUME:           raw_landing (in bronze schema)

CATALOG = catalog_name()                          # e.g., "myproject_demo"
CATALOG_LOC = DATABRICKS_CATALOG_LOCATION         # e.g., "s3://bucket/path"
VOL = LANDING_ZONE                                # e.g., "raw_landing"

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Catalog Creation

# COMMAND ----------

spark.sql(f"""
    CREATE CATALOG IF NOT EXISTS {CATALOG}
    MANAGED LOCATION '{CATALOG_LOC}'
""")
spark.sql(f"USE CATALOG {CATALOG}")
print_status("1/5", f"Catalog '{CATALOG}' created")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Schema Creation (Medallion 3-Layer)

# COMMAND ----------

for schema, comment in [
    (SCHEMA_BRONZE, "Raw ingestion layer - source system data as-is"),
    (SCHEMA_SILVER, "Cleansed layer - quality-checked and integrated data"),
    (SCHEMA_GOLD,   "Analytics layer - business logic applied, aggregated data"),
]:
    spark.sql(f"""
        CREATE SCHEMA IF NOT EXISTS {CATALOG}.{schema}
        COMMENT '{comment}'
    """)

print_status("2/5", f"Schemas created ({SCHEMA_BRONZE} / {SCHEMA_SILVER} / {SCHEMA_GOLD})")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Volume Creation (Raw Data Landing Zone)

# COMMAND ----------

spark.sql(f"""
    CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA_BRONZE}.{VOL}
    COMMENT 'Raw file landing zone for CSV/JSON ingestion'
""")
print_status("3/5", f"Volume '{VOL}' created")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Subdirectory Creation

# COMMAND ----------

import os

# CUSTOMIZE: Add or remove subdirectories based on your data sources
subdirs = ["customers", "accounts", "transactions", "campaigns", "campaign_responses", "web_events"]
vol_path = volume_path()

for d in subdirs:
    path = f"{vol_path}/{d}"
    os.makedirs(path, exist_ok=True)

print_status("4/5", f"Volume subdirectories created ({len(subdirs)} dirs)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Verification

# COMMAND ----------

result = spark.sql(f"SHOW SCHEMAS IN {CATALOG}").collect()
schemas = [r.databaseName for r in result]
assert SCHEMA_BRONZE in schemas, f"Missing schema: {SCHEMA_BRONZE}"
assert SCHEMA_SILVER in schemas, f"Missing schema: {SCHEMA_SILVER}"
assert SCHEMA_GOLD in schemas,   f"Missing schema: {SCHEMA_GOLD}"

print_status("5/5", "Verification passed - all resources created successfully")
print(f"\n  Catalog  : {CATALOG}")
print(f"  Schemas  : {', '.join(schemas)}")
print(f"  Volume   : {vol_path}")
